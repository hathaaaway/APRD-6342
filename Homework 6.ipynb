{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 6: Using LASSO Regression to Model Fat Sales Data\n",
    "Hathaway Zhang <br>\n",
    "104369396 <br>\n",
    "Oct.4, 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LassoLarsCV\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># Purchases</th>\n",
       "      <th>B01001001</th>\n",
       "      <th>B01001002</th>\n",
       "      <th>B01001003</th>\n",
       "      <th>B01001004</th>\n",
       "      <th>B01001005</th>\n",
       "      <th>B01001006</th>\n",
       "      <th>B01001007</th>\n",
       "      <th>B01001008</th>\n",
       "      <th>B01001009</th>\n",
       "      <th>...</th>\n",
       "      <th>B19001008</th>\n",
       "      <th>B19001009</th>\n",
       "      <th>B19001010</th>\n",
       "      <th>B19001011</th>\n",
       "      <th>B19001012</th>\n",
       "      <th>B19001013</th>\n",
       "      <th>B19001014</th>\n",
       "      <th>B19001015</th>\n",
       "      <th>B19001016</th>\n",
       "      <th>B19001017</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>206252</td>\n",
       "      <td>469.226965</td>\n",
       "      <td>31.432422</td>\n",
       "      <td>35.219052</td>\n",
       "      <td>33.628765</td>\n",
       "      <td>20.121017</td>\n",
       "      <td>12.610787</td>\n",
       "      <td>6.734480</td>\n",
       "      <td>6.225394</td>\n",
       "      <td>...</td>\n",
       "      <td>49.409690</td>\n",
       "      <td>53.306757</td>\n",
       "      <td>42.318307</td>\n",
       "      <td>83.167229</td>\n",
       "      <td>89.249208</td>\n",
       "      <td>102.141470</td>\n",
       "      <td>52.872330</td>\n",
       "      <td>36.440765</td>\n",
       "      <td>23.446284</td>\n",
       "      <td>21.197485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>61399</td>\n",
       "      <td>486.538869</td>\n",
       "      <td>22.899396</td>\n",
       "      <td>21.531295</td>\n",
       "      <td>27.036271</td>\n",
       "      <td>16.808091</td>\n",
       "      <td>28.355511</td>\n",
       "      <td>18.192479</td>\n",
       "      <td>13.534422</td>\n",
       "      <td>...</td>\n",
       "      <td>59.231680</td>\n",
       "      <td>50.093078</td>\n",
       "      <td>40.700626</td>\n",
       "      <td>92.612963</td>\n",
       "      <td>117.363344</td>\n",
       "      <td>113.344051</td>\n",
       "      <td>75.774243</td>\n",
       "      <td>33.000508</td>\n",
       "      <td>33.169741</td>\n",
       "      <td>24.792689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>73170</td>\n",
       "      <td>489.859232</td>\n",
       "      <td>28.905289</td>\n",
       "      <td>36.271696</td>\n",
       "      <td>28.235616</td>\n",
       "      <td>21.566216</td>\n",
       "      <td>12.218122</td>\n",
       "      <td>7.243406</td>\n",
       "      <td>7.380074</td>\n",
       "      <td>...</td>\n",
       "      <td>63.996993</td>\n",
       "      <td>47.322923</td>\n",
       "      <td>42.505211</td>\n",
       "      <td>70.420610</td>\n",
       "      <td>90.033143</td>\n",
       "      <td>98.677692</td>\n",
       "      <td>54.703249</td>\n",
       "      <td>20.125056</td>\n",
       "      <td>11.890525</td>\n",
       "      <td>16.537397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>251724</td>\n",
       "      <td>505.585483</td>\n",
       "      <td>32.054949</td>\n",
       "      <td>31.757004</td>\n",
       "      <td>28.102207</td>\n",
       "      <td>18.651380</td>\n",
       "      <td>12.080692</td>\n",
       "      <td>7.035483</td>\n",
       "      <td>7.686991</td>\n",
       "      <td>...</td>\n",
       "      <td>54.790900</td>\n",
       "      <td>48.681562</td>\n",
       "      <td>43.873381</td>\n",
       "      <td>84.717507</td>\n",
       "      <td>112.204444</td>\n",
       "      <td>127.137252</td>\n",
       "      <td>83.019904</td>\n",
       "      <td>43.731067</td>\n",
       "      <td>38.851729</td>\n",
       "      <td>40.427349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>37382</td>\n",
       "      <td>495.586111</td>\n",
       "      <td>25.413301</td>\n",
       "      <td>29.318924</td>\n",
       "      <td>26.162324</td>\n",
       "      <td>19.260607</td>\n",
       "      <td>12.893906</td>\n",
       "      <td>6.580707</td>\n",
       "      <td>7.062222</td>\n",
       "      <td>...</td>\n",
       "      <td>58.883378</td>\n",
       "      <td>51.761414</td>\n",
       "      <td>47.310187</td>\n",
       "      <td>81.902582</td>\n",
       "      <td>93.793717</td>\n",
       "      <td>130.103014</td>\n",
       "      <td>71.982704</td>\n",
       "      <td>36.118530</td>\n",
       "      <td>31.603714</td>\n",
       "      <td>19.648989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 190 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   # Purchases  B01001001   B01001002  B01001003  B01001004  B01001005  \\\n",
       "0           22     206252  469.226965  31.432422  35.219052  33.628765   \n",
       "1            7      61399  486.538869  22.899396  21.531295  27.036271   \n",
       "2            3      73170  489.859232  28.905289  36.271696  28.235616   \n",
       "3           94     251724  505.585483  32.054949  31.757004  28.102207   \n",
       "4            0      37382  495.586111  25.413301  29.318924  26.162324   \n",
       "\n",
       "   B01001006  B01001007  B01001008  B01001009    ...      B19001008  \\\n",
       "0  20.121017  12.610787   6.734480   6.225394    ...      49.409690   \n",
       "1  16.808091  28.355511  18.192479  13.534422    ...      59.231680   \n",
       "2  21.566216  12.218122   7.243406   7.380074    ...      63.996993   \n",
       "3  18.651380  12.080692   7.035483   7.686991    ...      54.790900   \n",
       "4  19.260607  12.893906   6.580707   7.062222    ...      58.883378   \n",
       "\n",
       "   B19001009  B19001010  B19001011   B19001012   B19001013  B19001014  \\\n",
       "0  53.306757  42.318307  83.167229   89.249208  102.141470  52.872330   \n",
       "1  50.093078  40.700626  92.612963  117.363344  113.344051  75.774243   \n",
       "2  47.322923  42.505211  70.420610   90.033143   98.677692  54.703249   \n",
       "3  48.681562  43.873381  84.717507  112.204444  127.137252  83.019904   \n",
       "4  51.761414  47.310187  81.902582   93.793717  130.103014  71.982704   \n",
       "\n",
       "   B19001015  B19001016  B19001017  \n",
       "0  36.440765  23.446284  21.197485  \n",
       "1  33.000508  33.169741  24.792689  \n",
       "2  20.125056  11.890525  16.537397  \n",
       "3  43.731067  38.851729  40.427349  \n",
       "4  36.118530  31.603714  19.648989  \n",
       "\n",
       "[5 rows x 190 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfRaw = pd.read_csv(\"finalmaster-ratios.csv\")\n",
    "dfRaw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all the predictors you're going to feed into the LassoLarsCV model\n",
    "allvariablenames = list(dfRaw.columns.values)\n",
    "listofallpredictors = allvariablenames[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    22\n",
       "1     7\n",
       "2     3\n",
       "3    94\n",
       "4     0\n",
       "Name: # Purchases, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load predictors into dataframe\n",
    "predictors = dfRaw[listofallpredictors]  \n",
    "#load target into dataframe\n",
    "target = dfRaw['# Purchases']   \n",
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=1.496e+00, with an active set of 5 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.098e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=7.329e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 5.771e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.051e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 17 iterations, alpha=5.739e-01, previous alpha=5.739e-01, with an active set of 14 regressors.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 16 iterations, i.e. alpha=4.100e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 27 iterations, i.e. alpha=2.867e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 44 iterations, i.e. alpha=2.050e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.013e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 6.989e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.986e-01, previous alpha=1.960e-01, with an active set of 37 regressors.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=3.232e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 5.475e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.616e-01, with an active set of 43 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 60 iterations, i.e. alpha=1.295e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 62 iterations, alpha=1.281e-01, previous alpha=1.277e-01, with an active set of 53 regressors.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.439e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.037e+00, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.200e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 9.828e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=7.188e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 21 iterations, alpha=7.036e-01, previous alpha=6.809e-01, with an active set of 18 regressors.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=6.916e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=3.645e-01, with an active set of 17 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 22 iterations, i.e. alpha=3.456e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 46 iterations, i.e. alpha=1.867e-01, with an active set of 38 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.734e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 50 iterations, i.e. alpha=1.709e-01, with an active set of 40 regressors, and the smallest cholesky pivot element being 7.376e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 51 iterations, i.e. alpha=1.692e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 56 iterations, alpha=1.668e-01, previous alpha=1.607e-01, with an active set of 45 regressors.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.572e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=1.132e+00, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=7.640e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 7.525e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 26 iterations, i.e. alpha=3.863e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=3.560e-01, with an active set of 24 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 54 iterations, i.e. alpha=1.932e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 59 iterations, i.e. alpha=1.737e-01, with an active set of 41 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 64 iterations, alpha=1.702e-01, previous alpha=1.687e-01, with an active set of 45 regressors.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.644e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.178e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=9.316e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.155e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 32 iterations, alpha=3.203e-01, previous alpha=3.135e-01, with an active set of 25 regressors.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 23 iterations, i.e. alpha=2.100e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 39 iterations, i.e. alpha=1.080e-01, with an active set of 39 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.721e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 53 iterations, i.e. alpha=8.721e-02, with an active set of 49 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 61 iterations, alpha=7.637e-02, previous alpha=7.570e-02, with an active set of 56 regressors.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 3 iterations, i.e. alpha=1.442e+00, with an active set of 3 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=9.333e-01, with an active set of 6 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=8.051e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=7.193e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=4.572e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 15 iterations, i.e. alpha=4.446e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=2.382e-01, with an active set of 30 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 42 iterations, i.e. alpha=1.997e-01, with an active set of 32 regressors, and the smallest cholesky pivot element being 6.909e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 46 iterations, alpha=1.777e-01, previous alpha=1.768e-01, with an active set of 35 regressors.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=6.554e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.277e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=2.089e-01, with an active set of 28 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 38 iterations, i.e. alpha=1.880e-01, with an active set of 36 regressors, and the smallest cholesky pivot element being 7.598e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 43 iterations, alpha=1.742e-01, previous alpha=1.733e-01, with an active set of 38 regressors.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.372e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=9.825e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=6.732e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:313: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 14 iterations, i.e. alpha=5.285e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n",
      "  ConvergenceWarning)\n",
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:339: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 16 iterations, alpha=5.185e-01, previous alpha=5.185e-01, with an active set of 15 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test sets, with 30% retained for test\n",
    "pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123) \n",
    "#  (1) pred_train, the predictors training set \n",
    "#  (2) pred_test, the predictors test test \n",
    "#  (3) tar_train, the target training set \n",
    "#  (4) tar_test, the target test set.\n",
    "model = LassoLarsCV(cv=10, precompute=False).fit(pred_train, tar_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 1\n",
    "In your own words, explain what the above lines of code are doing. Why am I doing it? Explain each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B01001014' 0.8557934663949708]\n",
      "['B01001036' 2.5053958893202446]\n",
      "['B01001037' 0.8894158553356212]\n",
      "['B01001038' 1.5315800095865324]\n",
      "['B02001005' 0.41254033933206735]\n",
      "['B13014026' 0.4800235646035068]\n",
      "['B13014027' 0.6977472755369424]\n",
      "['B13016001' 874929430.0943857]\n",
      "['B19001017' 1.4834469570888447]\n"
     ]
    }
   ],
   "source": [
    "# build coefficent chart\n",
    "# Question 1\n",
    "# load predictors into predictors_model\n",
    "predictors_model=pd.DataFrame(listofallpredictors)\n",
    "# rename header of predictors_model as 'label'\n",
    "predictors_model.columns = ['label']\n",
    "# create a column named coeff and add coeff. to dataframe\n",
    "predictors_model['coeff'] = model.coef_\n",
    "# check all the index and row in predictors\n",
    "for index, row in predictors_model.iterrows():\n",
    "    # since any coefficients that are non-zero are significant, find all the coeff, greater than 0\n",
    "    if row['coeff'] > 0:\n",
    "        # print out the significant values\n",
    "        print(row.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "Interpret each variable intuitively. What Census variables most predict sales? What does that mean, practically? Here's what I'd put for the example above. \"In areas where there are more females aged 30-34, we sell more Bobo Bars.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['B01001014' 0.8557908775529921] Males aged 40 to 44 Years. <br>\n",
    "In areas where there are more males aged 40-44, we sell 0.8557908775529921 unit more Bobo Bars\n",
    "<br>\n",
    "<br>\n",
    "['B01001036' 2.505392496591849] Females aged 30 to 34 Years. <br>\n",
    "In areas where there are more females aged 30-34, we sell 2.505392496591849 unit more Bobo Bars\n",
    "<br>\n",
    "<br>\n",
    "['B01001037' 0.8894214357013622] Females aged 35 to 39 Years. <br>\n",
    "In areas where there are more females aged 35-39, we sell 0.8894214357013622] unit more Bobo Bars\n",
    "<br>\n",
    "<br>\n",
    "['B01001038' 1.5315839680821497] Females aged 40 to 44 Years. <br>\n",
    "In areas where there are more females aged 40-44, we sell 1.5315839680821497 unit more Bobo Bars\n",
    "<br>\n",
    "<br>\n",
    "['B02001005' 0.4125408937426837] Asian Alone. <br>\n",
    "In areas where there are asian alone, we sell 0.4125408937426837 unit more Bobo Bars\n",
    "<br>\n",
    "<br>\n",
    "['B13014026' 0.4800240326923769] Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Bachelor's Degree. <br>\n",
    "In areas where there are more females aged 15-50 that had a birth in the past 12 months with Bachelor's degree, we sell 0.4800240326923769 unit more Bobo Bars\n",
    "<br>\n",
    "<br>\n",
    "['B13014027' 0.6977454940063235] Women 15 to 50 Years Who Had a Birth in the Past 12 Months with Graduate or Professional Degree. <br>\n",
    "In areas where there are more females aged 15-50 that had a birth in the past 12 months with graduate or professional degree, we sell 0.6977454940063235 unit more Bobo Bars\n",
    "<br>\n",
    "<br>\n",
    "['B13016001' 874922971.7249781] Women 15 to 50 Years Who Had a Birth in the Past 12 Months. <br>\n",
    "In areas where there are more females aged 15-50 that had a birth in the past 12 months, we sell 874922971.7249781 unit more Bobo Bars\n",
    "<br>\n",
    "<br>\n",
    "['B19001017' 1.4834465563617387] Household with income 200,000 or More. <br>\n",
    "In areas where there are more household with income 200,000 or more, we sell 1.4834465563617387 unit more Bobo Bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 3\n",
    "If I had to report only two census variables to my boss that most steeply predicted sales, what would those be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the higher the value of the coefficient, the steeper the relationship between sales and that variable.<br>\n",
    "I will report B13016001 and B01001036 to my boss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 4\n",
    "Run the code above, then do the same thing for your test sets (2) and (4) above. Compare the two outputs. Are the training and text set mean squared errors similar? What does that mean practically? Think back to your stats class, or Google it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data MSE\n",
      "22025.320770047212\n"
     ]
    }
   ],
   "source": [
    "train_error_train = mean_squared_error(tar_train, model.predict(pred_train))\n",
    "print ('training data MSE')\n",
    "print(train_error_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing data MSE\n",
      "41549.13946199703\n"
     ]
    }
   ],
   "source": [
    "train_error_test = mean_squared_error(tar_test, model.predict(pred_test))\n",
    "print ('testing data MSE')\n",
    "print(train_error_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean squared error of training data set is different from the mean squared error of testing data set. Since the Mean Squared Error is a measure of how close a fitted line is to data points and the smaller the Mean Squared Error, the closer the fit is to the data, we could know that the training data set fits the model better than the testing data set does. \n",
    "\n",
    "(citation: https://www.vernier.com/til/1014/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same process for the test set. Compare the two R squareds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data R-square\n",
      "0.24002799797602614\n"
     ]
    }
   ],
   "source": [
    "#r squared\n",
    "rsquared_train=model.score(pred_train,tar_train)\n",
    "print ('training data R-square')\n",
    "print(rsquared_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing data R-square\n",
      "0.17587095531914365\n"
     ]
    }
   ],
   "source": [
    "#r squared\n",
    "rsquared_test=model.score(pred_test,tar_test)\n",
    "print ('testing data R-square')\n",
    "print(rsquared_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is a better regression model as it has larger R-square value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 5\n",
    "If your boss asked, \"How well does Census data, overall, predict sales?\" What would you say? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, Census data does a bad job predicting sales. In both MSE test and R-square test, the training date set does a better job fitting the model than the practical testing data set does. However, even within the training date set, the R-square value is still to small to make proper prediction. The R-square value (around 20%), tells the variance of its errors is 20% less than the variance of the dependent variable and the sd of its error is 11% less than the sd of the dependent variable. Therefore, the sd of the regression modelâ€™s errors is only about 1/10 the size of the sd of the errors that you would get with a constant-only model. Along with the fact that the MSE of both dataset is relatively large, we could conclude that the Census data couldn't predict the sales properly.\n",
    "\n",
    "(citation: https://people.duke.edu/~rnau/rsquared.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QUESTION 6\n",
    "Finally, let's see what our y-intercept is, so we can interpret what our baseline sales number looks like, all things considered: What is our baseline sales number? What does that mean, practically? Think back to what y-intercepts mean in regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y interecept:\n",
      "22.194675187960804\n"
     ]
    }
   ],
   "source": [
    "print(\"y interecept:\")\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y-intercept is 22.194675187960804 in this cse. When x = 0, the corresponding y-value is the y-intercept. In this case, we could know that the baseline sales number is 22.194675187960804, which means when variables that truly predict unique amounts of sales equal to 0, the baseline sales are 22.194675187960804."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
